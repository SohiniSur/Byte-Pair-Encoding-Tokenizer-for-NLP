# -*- coding: utf-8 -*-
"""Bengali_BPE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15X55Dn_Ct-7l_-9hdbek4t0yqHxBiAIR
"""

!pip install --upgrade huggingface_hub

from datasets import load_dataset

ds = load_dataset("ai4bharat/samanantar", "bn", split="train")

# @title
#from datasets import load_dataset

#ds = load_dataset("nyu-mll/glue", "cola")

print(ds)

print(ds.features)

for i in range(10):
    print(ds['tgt'][i])
#print(ds['tgt'][0])

tgt_texts=ds['tgt'][:10]
full_text = " ".join(tgt_texts)
print(full_text)

# @title
# Write all tgt rows into one file
with open("corpus.txt", "w", encoding="utf-8") as f:
    for line in ds["tgt"]:   # ds is just one Dataset (not DatasetDict)
        f.write(line + "\n")

# Save to Google Drive
!cp corpus.txt /content/drive/MyDrive/

from google.colab import drive
drive.mount('/content/drive')

# @title
batch_size = 100000   # adjust if needed

with open("Train_txt.txt", "w", encoding="utf-8") as f:
    for i in range(0, len(ds), batch_size):
        # select a slice of the dataset
        batch = ds[i : i + batch_size]["tgt"]

        # write each line to file
        for line in batch:
            f.write(line + "\n")

print("✅ Finished writing Train_txt.txt")
# Save to Google Drive
#!cp train_text.txt /content/drive/MyDrive/
!cp /content/Train_txt.txt /content/drive/MyDrive/Train_txt.txt

# @title
with open("Train_txt.txt", "r", encoding="utf-8") as f:
    for _ in range(10):   # first 10 lines
        print(f.readline().strip())

# @title
with open("Train_txt.txt", "r", encoding="utf-8") as f:
    text = f.read()

print(text[:1000])   # first 1000 characters
print("-----")
print(text[-1000:])  # last 1000 characters

# @title
!head Train_txt.txt

# @title
max_tokens = 10_000_000  # safe for Colab (≈3–4 GB RAM)
tokens = []
batch_size = 10000  # number of lines per batch, you can tune this

with open("Train_txt.txt", "r", encoding="utf-8") as f:


    batch = []
    for i, line in enumerate(f, 1):
      while(len(tokens) < max_tokens):
          batch.append(line.strip())

          if i % batch_size == 0:  # process the batch
              text_batch = "\n".join(batch)               # join into one string
              batch_tokens = list(text_batch.encode("utf-8"))  # encode batch
              tokens.extend(batch_tokens)
              batch = []  # reset

    # process the remaining lines (last batch)
      '''if batch:
          text_batch = "\n".join(batch)
          batch_tokens = list(text_batch.encode("utf-8"))
          tokens.extend(batch_tokens)'''

# @title
max_tokens = 10_000_000  # safe for Colab (≈3–4 GB RAM)

tokens = []
with open("corpus.txt", "r", encoding="utf-8") as f:
    for line in f:
        line_tokens = list(line.encode("utf-8"))
        if len(tokens) + len(line_tokens) > max_tokens:
            tokens.extend(line_tokens[:max_tokens - len(tokens)])
            break
        else:
            tokens.extend(line_tokens)

print(f"Total tokens loaded: {len(tokens):,}")

"""# BPE Encoder Implementation"""



max_tokens = 50_000_000
tokens = []
batch_size = 10_000       # number of lines per batch

with open("/content/drive/MyDrive/Train_txt.txt", "r", encoding="utf-8") as f:
    batch = []
    for i, line in enumerate(f, 1):
        batch.append(line.strip())

        # process every batch_size lines
        if i % batch_size == 0:
            text_batch = "\n".join(batch)                   # join into one string
            batch_tokens = list(text_batch.encode("utf-8")) # encode to bytes
            tokens.extend(batch_tokens)
            batch = []                                      # reset batch

            # stop if we reached max_tokens
            if len(tokens) >= max_tokens:
                tokens = tokens[:max_tokens]  # trim extra
                break

    # process leftover batch if not empty
    if batch and len(tokens) < max_tokens:
        text_batch = "\n".join(batch)
        batch_tokens = list(text_batch.encode("utf-8"))
        tokens.extend(batch_tokens[:max_tokens - len(tokens)])

print(f"✅ Loaded {len(tokens):,} tokens")

print(len(tokens))

print(tokens[:10])

"""# using ord"""

max_tokens = 50_000_000
tokens = []
batch_size = 10_000       # number of lines per batch

with open("/content/drive/MyDrive/Train_txt.txt", "r", encoding="utf-8") as f:
    batch = []
    for i, line in enumerate(f, 1):
        batch.append(line.rstrip("\n"))   # keep characters, remove newline

        # process every batch_size lines
        if i % batch_size == 0:
            text_batch = "\n".join(batch)     # join into one string
            batch_tokens = [ord(ch) for ch in text_batch]  # convert each char → int
            tokens.extend(batch_tokens)
            batch = []                         # reset batch

            # stop if we reached max_tokens
            if len(tokens) >= max_tokens:
                tokens = tokens[:max_tokens]
                break

    # process leftover batch if any
    if batch and len(tokens) < max_tokens:
        text_batch = "\n".join(batch)
        batch_tokens = [ord(ch) for ch in text_batch]
        remaining = max_tokens - len(tokens)
        tokens.extend(batch_tokens[:remaining])

print(f"✅ Loaded {len(tokens):,} tokens")

max(tokens)

b=tokens.copy()
b.sort()

ord('৾')

print('৾'.encode('utf-8'))

print(tokens[:100])

def get_stats(ids):
    counts = {}
    for pair in zip(ids, ids[1:]):
        counts[pair] = counts.get(pair, 0) + 1
    return counts

def merge(ids, pair, idx):
  newids = []
  i = 0
  while i < len(ids):
    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:
      newids.append(idx)
      i += 2
    else:
      newids.append(ids[i])
      i += 1
  return newids

ids=list(tokens)

# ---
#vocab_size = 2588 # the desired final vocabulary size (this will be adjusted based on new max(ext) and num_merges)
#num_merges = vocab_size - 256
num_merges = 30
ids = list(tokens) # copy so we don't destroy the original list


merges = {} # (int, int) -> int
for i in range(num_merges):
  stats = get_stats(ids)
  pair = max(stats, key=stats.get)
  # Assign new token ID, making sure it's distinct from all base characters
  idx = 2558 + i
  print(f"merging {pair} into a new token {idx}")
  ids = merge(ids, pair, idx)
  merges[pair] = idx

merges

# --- CONFIGURATION ---
BASE_VOCAB = 2558             # starting ID for merges
TARGET_MERGES = 30000         # OR choose any number (e.g., 30k, 50k)
FREQ_THRESHOLD = 100          # stop if top pair occurs <= 100 times

# --- INITIAL SETUP ---
ids = list(tokens)            # do not modify original token list
merges = {}                   # store (pair -> new token)

# --- MERGING LOOP ---
j = 0
while j < TARGET_MERGES:

    stats = get_stats(ids)
    if not stats:
        break

    # find most frequent pair and its count
    (top_pair, top_count) = max(stats.items(), key=lambda x: x[1])

    # ---- EARLY STOP CONDITION ----
    if top_count <= FREQ_THRESHOLD:
        print(f"Stopping early: top pair {top_pair} occurs only {top_count} times.")
        break

    # assign new token ID
    new_id = BASE_VOCAB + j
    print(f"Merging {top_pair} (count={top_count}) → new token {new_id}")

    # perform merge
    ids = merge(ids, top_pair, new_id)
    merges[top_pair] = new_id

    j += 1

print(f"\nTotal merges performed: {j}")
print(f"Final vocabulary size: {BASE_VOCAB + j}")

print("tokens length:", len(tokens))
print("ids length:", len(ids))
print(f"compression ratio: {len(tokens) / len(ids):.2f}X")

ord('ঀ')

import string

# Ensure all unique 'ord' values from the 'tokens' list are included,
# as well as explicit digits, punctuation, and space.
# The 'tokens' list (from cell 07FG2qq1RY-_) contains all ordinals from the text.
initial_char_codes = set(tokens)

# Add standard ASCII characters if they aren't already in tokens
explicit_ascii_chars = set([ord(ch) for ch in string.digits + string.punctuation] + [32])
ext = sorted(list(initial_char_codes.union(explicit_ascii_chars)))

vocab = {idx: chr(idx).encode("utf-8") for idx in ext}

for (p0,p1), idx in merges.items():
    vocab[idx] = vocab[p0] + vocab[p1]

def decode(ids):
    s=''
    tokens = b"".join(vocab[i] for i in ids)
    return tokens.decode('utf-8', errors='replace')
#decode([])

vocab = {idx: chr(idx).encode("utf-8") for idx in ext}
for (p0, p1), idx in merges.items():
    vocab[idx] = vocab[p0] + vocab[p1]

def decode(ids):
  # given ids (list of integers), return Python string
  tokens = b"".join(vocab[idx] for idx in ids)
  text = tokens.decode("utf-8", errors="replace")
  return text

def encode(text):
  # given a string, return list of integers (the tokens)
  tokens = list(text.encode("utf-8"))
  while len(tokens) >= 2:
    stats = get_stats(tokens)
    pair = min(stats, key=lambda p: merges.get(p, float("inf")))
    if pair not in merges:
      break # nothing else can be merged
    idx = merges[pair]
    tokens = merge(tokens, pair, idx)
  return tokens

#print(encode(""))



def encode(text):
  # given a string, return list of integers (the tokens)
  tokens = [ord(i) for i in text]
  while len(tokens) >= 2:
    stats = get_stats(tokens)
    pair = min(stats, key=lambda p: merges.get(p, float("inf")))
    if pair not in merges:
      break # nothing else can be merged
    idx = merges[pair]
    tokens = merge(tokens, pair, idx)
  return tokens

#print(encode(""))

test_text="""অন‍্য দিকে, ফ্রান্সের প্রেসিডেন্ট ইমানুয়েল মাক্রোঁ বলেন, “ইউরোপীয় নেতাদের সঙ্গে ভার্চুয়াল বৈঠকে ট্রাম্প পরিষ্কার করে জানান, তিনি রাশিয়া-ইউক্রেনের যুদ্ধ থামাতে তৎপর। আগামী শুক্রবার আলাস্কায় পুতিনের সঙ্গে বৈঠকে মূলত রুশ-ইউক্রেন যুদ্ধবিরতি নিয়েই আলোচনা হবে।”

বুধবার, ইউরোপীয় নেতা এবং ট্রাম্পের উদ্দেশে জ়েলেনস্কি বলেন, “শুক্রবারের বৈঠকের আগে পুতিন সকলকে ধাঁধায় রাখতে চাইছেন।” তাঁর সংযোজন, “পুতিন সকলের উপরে চাপ সৃষ্টি করে বোঝাতে চাইছেন ইউক্রেনের উপর রাশিয়া ক্ষমতা বিস্তার করতে সক্ষম।” বুধবার জ়েলেনস্কির সঙ্গে বৈঠকের পরেই রাশিয়ার উদ্দেশে ট্রাম্পের ‘হুমকি’ যথেষ্ট তাৎপর্যপূর্ণ।"""

print(encode(test_text))

test_tokens=[ord(i) for i in test_text]
print("compression ratio=", len(test_tokens)/len(encode(test_text)))

print(decode(encode(test_text)) == test_text)

# @title
tokens = []
with open("Train_txt.txt", "r", encoding="utf-8") as f:
    for line in f:
        line_tokens = list(line.encode("utf-8"))  # only this line
        tokens.extend(line_tokens)                # append

# @title
tokens = text.encode("utf-8") # raw bytes
tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience

"""# REGEX"""

# @title
import regex as re
gpt2pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")

print(re.findall(gpt2pat, "Hello've world123 how's are you!!!?"))

import regex as re

bengali_pat = re.compile(
    r""" ?\p{Bengali}+| ?\p{N}+| ?[^\s\p{Bengali}\p{N}]+|\s+(?!\S)|\s+"""
)

text = "আমি বাংলা123 ভালোবাসি!!! তুমি কেমন আছো?"
print(re.findall(bengali_pat, text))



def encode_re(text):
    tokens = []

    for token in re.findall(bengali_pat, text):
        t = [ord(c) for c in token]

        while len(t) > 1:
            stats = get_stats(t)
            pair  = min(stats, key=lambda x: merges.get(x, float('inf')))
            if pair not in merges:
                break
            idx = merges[pair]
            t = merge(t, pair, idx)
        tokens.extend(t)

    return tokens

encode_re(test_text)

test_tokens=[ord(i) for i in test_text]
print("compression ratio=", len(test_tokens)/len(encode_re(test_text)))

print("Visualizing Merged Tokens:")
print("-------------------------")
for (p0, p1), idx in merges.items():
    char_p0 = vocab[p0].decode('utf-8', errors='replace')
    char_p1 = vocab[p1].decode('utf-8', errors='replace')
    char_idx = vocab[idx].decode('utf-8', errors='replace')
    print(f"Merge: '{char_p0}' ({p0}) + '{char_p1}' ({p1}) -> '{char_idx}' ({idx})")